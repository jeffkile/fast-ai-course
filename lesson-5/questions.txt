1. Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU?
We use the CPU to change each image into a uniform larger size that allows for there to be spare margin around the image to allow for augmentation transforms. Then the GPU is used to apply augmentations to a batch all at once.

2. If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book's website for suggestions.
I am very familiar with regex

3. What are the two ways in which data is most commonly provided, for most deep learning datasets?
Either as individual files representing items of data possibly organized in folders. Or as a CSV where each row is an item, possibly with a filename to other text documents or images.

4. Look up the documentation for L and try using a few of the new methods that it adds.
x = [1, 2, 3]
y = L(x)
z = y.argwhere(ge(2)) 
a = z * 2
a

5. Look up the documentation for the Python pathlib module and try using a few methods of the Path class.
from pathlib import Path
my_path = Path.cwd()
new_path = my_path/'new_path'
new_path

6. Give two examples of ways that image transformations can degrade the quality of the data.
1. Rotating data in such a way that a large portion of the remaining image is black pixels with no data
2. Reducing the size of the image to a point where theres not enough pixels to train off of

7. What method does fastai provide to view the data in a DataLoaders?
show_batch - shows one batch of images in a grid of rows and columns

8. What method does fastai provide to help you debug a DataBlock?
summary - a log of all the operations performed on the data

9. Should you hold off on training a model until you have thoroughly cleaned your data?
No its good to train a model first so that you can see the confusion matrix and see which images are not being classified correctly, this will help you find the types of bad input images.

10. What are the two pieces that are combined into cross-entropy loss in PyTorch?
1. log_softmax - maps each category prediction into a list of values that add up to 1, we use the log of softmax because its easier to add them together instead of multiplying
2. nll_loss - negative log likelihood loss function which uses the values from log_softmax

11. What are the two properties of activations that softmax ensures? Why is this important?
1. Ensures all the probabilities add up to 1
2. Amplifies small differences so that its likely you will have one best choice

12. When might you want your activations to not have these two properties?
You might want your model to tell you it doesn't recognize any of the classes and not pick one

13. Calculate the exp and softmax columns of <<bear_softmax>> yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).
import math

teddy = 0.02
grizzly = -2.49
brown = 1.25

e_to_teddy = math.e**teddy
e_to_grizzly = math.e**grizzly
e_to_brown = math.e**brown

e_to_teddy, e_to_grizzly, e_to_brown
(1.0202013400267558, 0.08290996657517267, 3.490342957461841)

sum = e_to_teddy + e_to_grizzly + e_to_brown
sum
4.593454264063769

softmax_teddy = e_to_teddy / sum
softmax_grizzly = e_to_grizzly / sum
softmax_brown = e_to_brown / sum

softmax_teddy, softmax_grizzly, softmax_brown
(0.22209894370956398, 0.018049590092538183, 0.7598514661978978)

total = softmax_teddy + softmax_grizzly + softmax_brown
total
1.0

14. Why can't we use torch.where to create a loss function for datasets where our label can have more than two categories?
Because they are not one hot encoded

15. What is the value of log(-2)? Why?
Its undefined because there are no valid values of x in the equation -2 = 10 ^ x which id the definition of the log fucntion

16. What are two good rules of thumb for picking a learning rate from the learning rate finder?
1. One order of magnitude smaller than the smallest loss 
2. Pick the last point where the loss was clearly decreasing

17. What two steps does the fine_tune method do?
1. Trains the randomly added layers for 1 epoch, with the other layers frozen
2. Unfreezes the other layers and trains them for the number of epochs requested

18. In Jupyter Notebook, how do you get the source code for a method or function?
Add ?? to the end of the function

19. What are discriminative learning rates?
Using smaller learning rates for the early layers of the network and larger rates for later layers.

20. How is a Python slice object interpreted when passed as a learning rate to fastai?
The first value is the learning rate for the first layer and the second value is the learning rate for the last layers. Layers inbetween get an equidistant proportional value.

21. Why is early stopping a poor choice when using 1cycle training?
Because we need to get passed the local minimum in the loss graph, reducing the learning rate further gets us passed this local minimum

22. What is the difference between resnet50 and resnet101?
resnet101 has more layers than resnet50

23. What does to_fp16 do?
It uses 16 bit floating point numbers instead of 32 bit floating point numbers, this allows fast ai to use nvidia's tensor cores on their GPUs